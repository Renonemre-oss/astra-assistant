# Sistema RAG - Retrieval-Augmented Generation

## üìã √çndice
1. [Vis√£o Geral](#vis√£o-geral)
2. [Componentes](#componentes)
3. [Instala√ß√£o](#instala√ß√£o)
4. [Uso B√°sico](#uso-b√°sico)
5. [Exemplos Avan√ßados](#exemplos-avan√ßados)
6. [API Reference](#api-reference)
7. [Melhores Pr√°ticas](#melhores-pr√°ticas)

---

## üéØ Vis√£o Geral

O sistema **RAG (Retrieval-Augmented Generation)** permite que o ALEX/JARVIS:

- üß† **Aprenda com documentos** (PDF, TXT, MD)
- üîç **Busque semanticamente** em conversas e documentos
- üíæ **Armazene embeddings** para recupera√ß√£o eficiente
- ü§ñ **Gere contexto enriquecido** para LLMs

### Benef√≠cios

- **Mem√≥ria de Longo Prazo**: Lembra de conversas anteriores
- **Aprendizado Contextual**: Aprende com documenta√ß√£o
- **Busca Inteligente**: Encontra informa√ß√µes por significado, n√£o apenas palavras-chave
- **Respostas Informadas**: Gera respostas baseadas em conhecimento espec√≠fico

---

## üîß Componentes

### 1. Vector Store (`vector_store.py`)
Armazena embeddings usando **ChromaDB**.

**Caracter√≠sticas:**
- Persist√™ncia em disco
- Busca por similaridade vetorial
- Suporte a metadados
- Filtros avan√ßados

### 2. Embeddings Manager (`embeddings_manager.py`)
Gera embeddings usando **Sentence Transformers**.

**Modelo Padr√£o:** `all-MiniLM-L6-v2`
- Dimens√£o: 384
- Velocidade: Alta
- Qualidade: Boa para uso geral

### 3. Document Processor (`document_processor.py`)
Processa documentos em chunks.

**Formatos Suportados:**
- PDF (`.pdf`)
- Texto (`.txt`)
- Markdown (`.md`)

**Processamento:**
- Chunking inteligente (quebra em senten√ßas)
- Sobreposi√ß√£o configur√°vel
- Metadados autom√°ticos

### 4. RAG System (`rag_system.py`)
Integra todos os componentes.

**Funcionalidades:**
- Adicionar documentos e textos
- Busca sem√¢ntica
- Gera√ß√£o de contexto
- Salvar conversas
- Estat√≠sticas do sistema

---

## üì¶ Instala√ß√£o

### Depend√™ncias Principais

```bash
pip install chromadb sentence-transformers PyPDF2
```

### Depend√™ncias Completas

Adicione ao `requirements.txt`:

```txt
# RAG - Retrieval Augmented Generation
chromadb==0.5.23          # Vector database
sentence-transformers==3.3.1  # Embeddings
PyPDF2==3.0.1             # PDF processing
```

### Instala√ß√£o Completa

```bash
# Ambiente virtual
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# Instalar depend√™ncias
pip install -r requirements.txt
```

---

## üöÄ Uso B√°sico

### Inicializa√ß√£o

```python
from jarvis.ai import get_rag_system

# Obter inst√¢ncia global
rag = get_rag_system()

# Verificar status
stats = rag.get_stats()
print(f"Ready: {stats['ready']}")
```

### Adicionar Conhecimento

```python
# Adicionar texto diretamente
rag.add_text(
    "Python √© uma linguagem de programa√ß√£o interpretada.",
    metadata={'category': 'programming'}
)

# Adicionar documento
from pathlib import Path
rag.add_document(Path("manual.pdf"))

# Adicionar diret√≥rio
rag.add_directory(Path("docs/"))
```

### Busca Sem√¢ntica

```python
# Busca simples
results = rag.search("O que √© Python?", n_results=5)

for result in results:
    print(f"Relev√¢ncia: {1-result['distance']:.2%}")
    print(f"Texto: {result['document']}")
    print(f"Fonte: {result['metadata'].get('source', 'N/A')}")
```

### Gerar Contexto para LLM

```python
# Gerar contexto enriquecido
query = "Como criar APIs com Python?"
context = rag.generate_context(
    query, 
    n_results=3,
    max_context_length=2000
)

# Usar com LLM
prompt = f"""
Contexto:
{context}

Pergunta: {query}

Resposta:
"""
```

### Salvar Conversas

```python
# Adicionar conversa ao RAG
rag.add_conversation(
    user_message="Qual √© o seu nome?",
    assistant_response="Meu nome √© ALEX!",
    metadata={'timestamp': '2024-01-01'}
)

# Buscar conversas antigas
results = rag.search("qual √© o nome do assistente")
```

---

## üí° Exemplos Avan√ßados

### Exemplo 1: Processar Documenta√ß√£o

```python
from pathlib import Path
from jarvis.ai import get_rag_system

rag = get_rag_system()

# Processar toda documenta√ß√£o
docs_dir = Path("project_docs/")
count = rag.add_directory(docs_dir)
print(f"‚úÖ Processados {count} chunks")

# Consultar documenta√ß√£o
results = rag.search("Como configurar o sistema?")

for i, result in enumerate(results[:3], 1):
    print(f"\n[{i}] {result['metadata']['source']}")
    print(f"    {result['document'][:200]}...")
```

### Exemplo 2: Base de Conhecimento Customizada

```python
# Carregar conhecimentos espec√≠ficos
conhecimentos = [
    {
        "text": "O comando 'git commit' salva mudan√ßas no reposit√≥rio.",
        "metadata": {"category": "git", "difficulty": "basic"}
    },
    {
        "text": "O comando 'git rebase' reescreve o hist√≥rico de commits.",
        "metadata": {"category": "git", "difficulty": "advanced"}
    }
]

for k in conhecimentos:
    rag.add_text(k["text"], metadata=k["metadata"])

# Buscar por dificuldade
results = rag.search(
    "comandos git",
    filters={"difficulty": "basic"}
)
```

### Exemplo 3: Integra√ß√£o com Assistant

```python
from jarvis.ai import get_rag_system

class SmartAssistant:
    def __init__(self):
        self.rag = get_rag_system()
    
    def process_message(self, user_message: str) -> str:
        # Buscar contexto relevante
        context = self.rag.generate_context(
            user_message, 
            n_results=3
        )
        
        # Gerar resposta (integrar com LLM)
        if context:
            prompt = f"Contexto:\n{context}\n\nPergunta: {user_message}"
            response = self.generate_response(prompt)
        else:
            response = self.generate_response(user_message)
        
        # Salvar conversa para futuro
        self.rag.add_conversation(user_message, response)
        
        return response
    
    def generate_response(self, prompt: str) -> str:
        # Integrar com seu LLM preferido
        pass
```

---

## üìö API Reference

### RAGSystem

#### `__init__(vector_store, embeddings_manager, document_processor)`
Inicializa sistema RAG.

**Par√¢metros:**
- `vector_store` (VectorStore, optional): Vector store customizado
- `embeddings_manager` (EmbeddingsManager, optional): Embeddings manager customizado
- `document_processor` (DocumentProcessor, optional): Document processor customizado

#### `add_text(text: str, metadata: dict) -> bool`
Adiciona texto ao sistema.

**Par√¢metros:**
- `text`: Texto para adicionar
- `metadata`: Metadados opcionais

**Retorna:** `True` se sucesso

#### `add_document(file_path: Path) -> bool`
Adiciona documento ao sistema.

**Par√¢metros:**
- `file_path`: Caminho do arquivo (.pdf, .txt, .md)

**Retorna:** `True` se sucesso

#### `add_directory(directory: Path) -> int`
Adiciona todos os documentos de um diret√≥rio.

**Par√¢metros:**
- `directory`: Diret√≥rio com documentos

**Retorna:** N√∫mero de chunks adicionados

#### `search(query: str, n_results: int, filters: dict) -> List[dict]`
Busca sem√¢ntica.

**Par√¢metros:**
- `query`: Texto de busca
- `n_results`: N√∫mero de resultados (padr√£o: 5)
- `filters`: Filtros de metadata (opcional)

**Retorna:** Lista de resultados com `document`, `metadata`, `distance`, `id`

#### `generate_context(query: str, n_results: int, max_context_length: int) -> str`
Gera contexto para LLM.

**Par√¢metros:**
- `query`: Pergunta do usu√°rio
- `n_results`: N√∫mero de documentos (padr√£o: 3)
- `max_context_length`: Tamanho m√°ximo em caracteres (padr√£o: 2000)

**Retorna:** Contexto formatado

#### `add_conversation(user_message: str, assistant_response: str, metadata: dict) -> bool`
Adiciona conversa ao sistema.

**Par√¢metros:**
- `user_message`: Mensagem do usu√°rio
- `assistant_response`: Resposta do assistente
- `metadata`: Metadados adicionais (opcional)

**Retorna:** `True` se sucesso

#### `clear() -> bool`
Limpa todos os dados do RAG.

**Retorna:** `True` se sucesso

#### `get_stats() -> dict`
Retorna estat√≠sticas do sistema.

**Retorna:** Dict com `vector_store`, `embeddings`, `ready`

---

## ‚úÖ Melhores Pr√°ticas

### Performance

1. **Batch Processing**: Adicione m√∫ltiplos documentos de uma vez
2. **Chunk Size**: Ajuste baseado no tipo de conte√∫do (500-1000 caracteres)
3. **Filtros**: Use metadados para buscas mais eficientes

### Qualidade

1. **Limpeza de Texto**: Remova ru√≠do antes de adicionar
2. **Metadados Ricos**: Adicione contexto √∫til (data, autor, categoria)
3. **Testes de Busca**: Valide resultados regularmente

### Seguran√ßa

1. **Valida√ß√£o de Entrada**: Verifique documentos antes de processar
2. **Sanitiza√ß√£o**: Remova informa√ß√µes sens√≠veis
3. **Backups**: Fa√ßa backup do vector store periodicamente

### Escalabilidade

1. **Persist√™ncia**: Use diret√≥rio persistente para ChromaDB
2. **Indexa√ß√£o**: Adicione documentos offline quando poss√≠vel
3. **Limpeza**: Remova documentos antigos ou irrelevantes

---

## üîÑ Fluxo T√≠pico

```
1. Inicializar RAG
   ‚Üì
2. Carregar Documenta√ß√£o
   ‚Üì
3. Usu√°rio faz pergunta
   ‚Üì
4. RAG busca contexto relevante
   ‚Üì
5. LLM gera resposta com contexto
   ‚Üì
6. Salvar conversa no RAG
   ‚Üì
7. Repetir 3-6
```

---

## üìä Configura√ß√£o do Sistema

### Chunk Size

```python
from jarvis.ai import DocumentProcessor

# Customizar processador
processor = DocumentProcessor(
    chunk_size=800,      # Maior para documentos t√©cnicos
    chunk_overlap=100    # Mais sobreposi√ß√£o
)
```

### Modelo de Embeddings

```python
from jarvis.ai import EmbeddingsManager

# Usar modelo diferente
embeddings = EmbeddingsManager(
    model_name="paraphrase-multilingual-MiniLM-L12-v2"  # Multil√≠ngue
)
```

### Persist√™ncia

```python
from jarvis.ai import VectorStore
from pathlib import Path

# Diret√≥rio customizado
vector_store = VectorStore(
    persist_directory=Path("/data/vector_store")
)
```

---

## üêõ Troubleshooting

### ChromaDB n√£o inicializa

```bash
# Reinstalar ChromaDB
pip uninstall chromadb
pip install chromadb==0.5.23
```

### Modelo de embeddings n√£o carrega

```bash
# Baixar modelo manualmente
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

### PDFs n√£o processam

```bash
# Verificar PyPDF2
pip install --upgrade PyPDF2
```

---

## üìà Monitoramento

```python
# Estat√≠sticas do sistema
stats = rag.get_stats()

print(f"Vector Store:")
print(f"  - Dispon√≠vel: {stats['vector_store']['available']}")
print(f"  - Total docs: {stats['vector_store']['total_documents']}")

print(f"Embeddings:")
print(f"  - Modelo: {stats['embeddings']['model_name']}")
print(f"  - Dimens√£o: {stats['embeddings']['embedding_dim']}")

print(f"Sistema: {'‚úÖ Pronto' if stats['ready'] else '‚ùå N√£o pronto'}")
```

---

## üîó Links √öteis

- [ChromaDB Docs](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)
- [PyPDF2 Docs](https://pypdf2.readthedocs.io/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

---

**Vers√£o:** 3.0  
**√öltima Atualiza√ß√£o:** Dezembro 2024
