# ü§ñ AI Providers - Astra AI Assistant

Este guia explica como configurar e usar diferentes provedores de IA com o Astra.

## üìã Vis√£o Geral

O Astra suporta m√∫ltiplos provedores de IA atrav√©s do **AI Engine**, permitindo:

- ‚úÖ Usar IA local (Ollama) ou remota (OpenAI)
- ‚úÖ Fallback autom√°tico entre provedores
- ‚úÖ Trocar de provedor sem modificar c√≥digo
- ‚úÖ Usar m√∫ltiplos provedores simultaneamente

## üéØ Provedores Dispon√≠veis

| Provedor | Tipo | Custo | Privacidade | Qualidade |
|----------|------|-------|-------------|-----------|
| **Ollama** | Local | Gr√°tis | üü¢ Total | üü° Boa |
| **OpenAI** | Remoto | üí∞ Pago | üî¥ Baixa | üü¢ Excelente |
| **Anthropic** | Remoto | üí∞ Pago | üî¥ Baixa | üü¢ Excelente |
| **Google** | Remoto | üí∞ Pago | üî¥ Baixa | üü¢ Excelente |

## üè† Ollama (Local)

### Por que Ollama?

- **Privacidade Total**: Tudo roda na sua m√°quina
- **Sem Custos**: Modelos open-source gratuitos
- **Offline**: Funciona sem internet
- **R√°pido**: Ap√≥s download, respostas instant√¢neas

### Instala√ß√£o

**Windows:**
```powershell
# Baixe o instalador de https://ollama.ai
# Execute o instalador
# Verifique a instala√ß√£o
ollama --version
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**macOS:**
```bash
brew install ollama
```

### Modelos Dispon√≠veis

| Modelo | Tamanho | RAM M√≠n. | Descri√ß√£o |
|--------|---------|----------|-----------|
| `llama3.2` | 2 GB | 8 GB | Vers√£o leve e r√°pida |
| `llama3.2:70b` | 40 GB | 64 GB | Vers√£o completa |
| `mistral` | 4 GB | 8 GB | √ìtimo para c√≥digo |
| `codellama` | 4 GB | 8 GB | Especializado em programa√ß√£o |
| `phi` | 1.6 GB | 4 GB | Muito r√°pido, menor qualidade |

### Baixar Modelos

```bash
# Modelo recomendado
ollama pull llama3.2

# Outros modelos
ollama pull mistral
ollama pull codellama
ollama pull phi

# Listar modelos instalados
ollama list

# Remover modelo
ollama rm llama3.2
```

### Configura√ß√£o

Edite `config/ai_config.yaml`:

```yaml
default_provider: ollama

providers:
  ollama:
    enabled: true
    model: llama3.2          # Modelo a usar
    url: http://localhost:11434  # URL do servidor
    timeout: 60              # Timeout em segundos
    max_retries: 3           # Tentativas em caso de erro
```

### Testar

```python
from Astra.ai import AIEngine
import yaml

with open('config/ai_config.yaml') as f:
    config = yaml.safe_load(f)

engine = AIEngine(config)
response = engine.generate("Ol√°! Como voc√™ est√°?")
print(response.content)
```

### Troubleshooting

**Erro: "Connection refused"**
```bash
# Iniciar servidor Ollama
ollama serve
```

**Erro: "Model not found"**
```bash
# Baixar o modelo
ollama pull llama3.2
```

**Resposta lenta:**
```yaml
# Use modelo menor
providers:
  ollama:
    model: phi  # Mais r√°pido, menor qualidade
```

---

## üåê OpenAI

### Por que OpenAI?

- **Qualidade Superior**: GPT-4 √© um dos melhores modelos
- **Sem Hardware**: Roda na nuvem
- **Setup R√°pido**: Apenas API key necess√°ria

### Custos

| Modelo | Input (por 1M tokens) | Output (por 1M tokens) |
|--------|----------------------|------------------------|
| `gpt-3.5-turbo` | $0.50 | $1.50 |
| `gpt-4` | $30.00 | $60.00 |
| `gpt-4-turbo` | $10.00 | $30.00 |

üí° **Dica**: Comece com `gpt-3.5-turbo` para testar.

### Instala√ß√£o

1. **Criar Conta**
   - Acesse [platform.openai.com](https://platform.openai.com)
   - Crie uma conta
   - Adicione m√©todo de pagamento

2. **Obter API Key**
   - V√° para [API Keys](https://platform.openai.com/api-keys)
   - Clique em "Create new secret key"
   - Copie a key (s√≥ aparece uma vez!)

3. **Instalar Biblioteca**
```bash
pip install openai
```

### Configura√ß√£o

**Op√ß√£o 1: Vari√°vel de Ambiente (Recomendado)**

```bash
# Linux/Mac
export OPENAI_API_KEY="sk-..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."

# Windows CMD
set OPENAI_API_KEY=sk-...
```

Edite `config/ai_config.yaml`:
```yaml
default_provider: openai

providers:
  openai:
    enabled: true
    model: gpt-3.5-turbo
    api_key: ${OPENAI_API_KEY}  # L√™ da vari√°vel de ambiente
    timeout: 60
    max_retries: 3
```

**Op√ß√£o 2: Direto no Config (Menos Seguro)**

```yaml
providers:
  openai:
    enabled: true
    model: gpt-3.5-turbo
    api_key: "sk-..."  # Sua chave aqui
```

### Modelos Dispon√≠veis

```python
# Listar modelos dispon√≠veis
from openai import OpenAI

client = OpenAI(api_key="sua-chave")
models = client.models.list()
for model in models.data:
    print(model.id)
```

### Controlar Custos

```yaml
providers:
  openai:
    model: gpt-3.5-turbo  # Modelo mais barato
    
defaults:
  max_tokens: 500  # Limitar resposta
  temperature: 0.7

# Ativar cache para evitar requisi√ß√µes duplicadas
cache_enabled: true
cache_ttl: 3600
```

### Troubleshooting

**Erro: "Invalid API key"**
- Verifique se copiou a key completa
- Verifique se a vari√°vel de ambiente est√° configurada

**Erro: "Rate limit exceeded"**
- Aguarde alguns minutos
- Considere upgrade no plano OpenAI

**Custo muito alto:**
- Use `gpt-3.5-turbo` em vez de `gpt-4`
- Ative cache
- Limite `max_tokens`

---

## üîÑ Fallback Entre Provedores

Configure fallback autom√°tico para m√°xima confiabilidade:

```yaml
default_provider: ollama

providers:
  ollama:
    enabled: true
    model: llama3.2
  
  openai:
    enabled: true
    model: gpt-3.5-turbo
    api_key: ${OPENAI_API_KEY}

# Se Ollama falhar, tenta OpenAI
fallback_chain:
  - ollama
  - openai
```

### Como Funciona

1. Astra tenta primeiro o `default_provider` (Ollama)
2. Se falhar (modelo n√£o encontrado, servidor offline, etc.)
3. Automaticamente tenta o pr√≥ximo na `fallback_chain` (OpenAI)
4. Se todos falharem, retorna erro

### Exemplo de Uso

```python
# Mesmo c√≥digo funciona com qualquer provedor
response = engine.generate("Ol√°!")

# Metadados mostram qual provedor foi usado
print(f"Provedor: {response.provider}")
print(f"Modelo: {response.model}")

# Se foi fallback
if response.metadata.get('is_fallback'):
    print(f"Fallback de {response.metadata['original_provider']}")
```

---

## üéõÔ∏è Par√¢metros Avan√ßados

### Temperature

Controla criatividade vs precis√£o:

```yaml
defaults:
  temperature: 0.0  # Muito determin√≠stico (bom para fatos)
  temperature: 0.7  # Balanceado (padr√£o)
  temperature: 1.0  # Muito criativo (bom para ideias)
```

### Max Tokens

Limita tamanho da resposta:

```yaml
defaults:
  max_tokens: null   # Sem limite
  max_tokens: 500    # ~375 palavras
  max_tokens: 1000   # ~750 palavras
```

### System Prompt

Define comportamento da IA:

```yaml
defaults:
  system_prompt: |
    Voc√™ √© o Astra, um assistente t√©cnico especializado.
    Responda sempre em portugu√™s de Portugal.
    Seja conciso e direto ao ponto.
```

---

## üîê Seguran√ßa

### Boas Pr√°ticas

‚úÖ **Fa√ßa:**
- Use vari√°veis de ambiente para API keys
- Adicione `config/*.yaml` ao `.gitignore`
- Rotacione API keys regularmente
- Use Ollama para dados sens√≠veis

‚ùå **N√£o Fa√ßa:**
- Commitar API keys no Git
- Compartilhar API keys
- Usar mesma key em m√∫ltiplos projetos

### Exemplo de .gitignore

```gitignore
# API Keys e configura√ß√µes sens√≠veis
config/ai_config.yaml
config/skills_config.yaml
.env
*.key
```

---

## üìä Compara√ß√£o de Uso

### Para Desenvolvimento

**Recomenda√ß√£o**: Ollama (llama3.2)
- Gr√°tis
- Privado
- R√°pido para testar

### Para Produ√ß√£o (Uso Leve)

**Recomenda√ß√£o**: Ollama + OpenAI (fallback)
```yaml
fallback_chain:
  - ollama      # 99% das requisi√ß√µes
  - openai      # Backup quando Ollama falha
```

### Para Produ√ß√£o (Alta Qualidade)

**Recomenda√ß√£o**: OpenAI (gpt-4)
- Melhor qualidade
- Mais confi√°vel
- Suporte empresarial

---

## üÜò Precisa de Ajuda?

- **Documenta√ß√£o Ollama**: [ollama.ai/docs](https://ollama.ai/docs)
- **Documenta√ß√£o OpenAI**: [platform.openai.com/docs](https://platform.openai.com/docs)
- **Issues**: [GitHub Issues](https://github.com/Renonemre-oss/astra-assistant/issues)

---

**üí° Dica Final**: Comece com Ollama para aprender e testar. Quando estiver confort√°vel, adicione OpenAI como fallback para m√°xima confiabilidade!


