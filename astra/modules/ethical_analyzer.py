#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ASTRA - Analisador √âtico
M√≥dulo respons√°vel por avaliar pedidos do usu√°rio e identificar potenciais riscos,
permitindo ao ASTRA expressar opini√µes e dar conselhos respons√°veis.
"""

import re
import json
import logging
from typing import Dict, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

class RiskLevel(Enum):
    """N√≠veis de risco identificados"""
    NONE = 0
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class RiskCategory(Enum):
    """Categorias de risco"""
    HEALTH = "saude"
    SAFETY = "seguranca"
    LEGAL = "legal"
    ETHICAL = "etico"
    FINANCIAL = "financeiro"
    PRIVACY = "privacidade"
    RELATIONSHIP = "relacionamento"
    ADDICTION = "vicio"

@dataclass
class RiskAssessment:
    """Resultado da an√°lise de risco"""
    level: RiskLevel
    category: RiskCategory
    concern: str
    alternative_suggestion: Optional[str] = None
    reasoning: Optional[str] = None

class EthicalAnalyzer:
    """Analisador √©tico para avaliar pedidos do usu√°rio"""
    
    def __init__(self):
        self.risk_patterns = self._load_risk_patterns()
        self.concern_responses = self._load_concern_responses()
        
    def _load_risk_patterns(self) -> Dict[RiskCategory, List[Dict]]:
        """Carrega padr√µes que indicam poss√≠veis riscos"""
        return {
            RiskCategory.HEALTH: [
                {
                    "patterns": [r"n[a√£]o.*dormir", r"n[a√£]o.*comer", r"parar.*medica[m√ßc][a√£]o", 
                               r"pular.*refei[c√ß][a√£]o", r"diet.*extrema", r"jejum.*prolongado"],
                    "level": RiskLevel.MEDIUM,
                    "concern": "Isso pode afetar sua sa√∫de",
                    "suggestion": "Que tal consultar um profissional de sa√∫de primeiro?"
                },
                {
                    "patterns": [r"autoles√£o", r"automutila[c√ß][a√£]o", r"n[a√£]o.*vale.*pena.*viver"],
                    "level": RiskLevel.CRITICAL,
                    "concern": "Estou muito preocupado com voc√™",
                    "suggestion": "Por favor, procure ajuda profissional imediatamente"
                }
            ],
            
            RiskCategory.SAFETY: [
                {
                    "patterns": [r"dirigir.*bebado", r"dirigir.*b[e√™]bado", r"beber.*dirigir", r"dirigir.*beber",
                               r"dirigir.*depois.*beber", r"dirigir.*mesmo.*beb", r"excesso.*velocidade", r"corrida.*rua"],
                    "level": RiskLevel.HIGH,
                    "concern": "Isso √© muito perigoso para voc√™ e outros",
                    "suggestion": "Use transporte p√∫blico ou chame um t√°xi/Uber"
                },
                {
                    "patterns": [r"escalar.*sem.*equipamento", r"nadar.*sozinho.*mar",
                               r"caminhar.*sozinho.*noite.*perigoso"],
                    "level": RiskLevel.MEDIUM,
                    "concern": "Isso pode ser arriscado",
                    "suggestion": "Considere levar algu√©m ou usar equipamentos de seguran√ßa"
                }
            ],
            
            RiskCategory.FINANCIAL: [
                {
                    "patterns": [r"apostar.*tudo", r"apostar.*todas.*economias", r"investir.*todas.*economias", 
                               r"empr[e√©]stimo.*agiotas", r"pagar.*d[i√≠]vida.*com.*cart[a√£]o", 
                               r"comprar.*n[a√£]o.*posso.*pagar", r"investir.*todas.*economia"],
                    "level": RiskLevel.HIGH,
                    "concern": "Isso pode prejudicar muito sua situa√ß√£o financeira",
                    "suggestion": "Que tal repensar e talvez consultar um conselheiro financeiro?"
                }
            ],
            
            RiskCategory.LEGAL: [
                {
                    "patterns": [r"baixar.*pirata", r"hackear", r"roubar", r"furtar",
                               r"falsificar.*documento", r"sonegar.*imposto"],
                    "level": RiskLevel.HIGH,
                    "concern": "Isso pode ter consequ√™ncias legais s√©rias",
                    "suggestion": "Recomendo procurar alternativas legais"
                }
            ],
            
            RiskCategory.RELATIONSHIP: [
                {
                    "patterns": [r"terminar.*sem.*conversar", r"trair", r"mentir.*para.*parceiro",
                               r"vingan√ßa.*ex", r"stalkar", r"perseguir"],
                    "level": RiskLevel.MEDIUM,
                    "concern": "Isso pode machucar pessoas que voc√™ se importa",
                    "suggestion": "Conversas honestas geralmente resolvem melhor os problemas"
                }
            ],
            
            RiskCategory.ADDICTION: [
                {
                    "patterns": [r"beber.*esquecer.*problemas", r"usar.*droga.*escapar",
                               r"apostar.*quando.*triste", r"comprar.*compulsivamente"],
                    "level": RiskLevel.MEDIUM,
                    "concern": "Isso pode se tornar um h√°bito prejudicial",
                    "suggestion": "Existem formas mais saud√°veis de lidar com essas emo√ß√µes"
                }
            ],
            
            RiskCategory.PRIVACY: [
                {
                    "patterns": [r"compartilhar.*senha", r"postar.*dados.*pessoais",
                               r"enviar.*fotos.*√≠ntimas", r"dar.*informa√ß√£o.*estranho"],
                    "level": RiskLevel.MEDIUM,
                    "concern": "Isso pode comprometer sua privacidade e seguran√ßa",
                    "suggestion": "Mantenha suas informa√ß√µes pessoais sempre protegidas"
                }
            ]
        }
    
    def _load_concern_responses(self) -> Dict[RiskLevel, List[str]]:
        """Carrega respostas baseadas no n√≠vel de preocupa√ß√£o"""
        return {
            RiskLevel.LOW: [
                "Hmm, talvez seja melhor pensar um pouco mais sobre isso.",
                "N√£o tenho certeza se essa √© a melhor abordagem.",
                "Voc√™ j√° considerou outras op√ß√µes?"
            ],
            RiskLevel.MEDIUM: [
                "Estou um pouco preocupado com essa ideia.",
                "Acho que isso pode n√£o acabar bem.",
                "Voc√™ tem certeza de que quer fazer isso?",
                "Posso sugerir uma alternativa mais segura?"
            ],
            RiskLevel.HIGH: [
                "Estou realmente preocupado com voc√™.",
                "Sinceramente, n√£o acho que deveria fazer isso.",
                "Isso me deixa muito desconfort√°vel.",
                "Por favor, reconsidere essa decis√£o."
            ],
            RiskLevel.CRITICAL: [
                "Estou extremamente preocupado com voc√™.",
                "Por favor, n√£o fa√ßa isso.",
                "Voc√™ √© importante e sua seguran√ßa √© minha prioridade.",
                "Preciso insistir que procure ajuda profissional."
            ]
        }
    
    def analyze_request(self, user_input: str, context: Dict = None) -> Optional[RiskAssessment]:
        """
        Analisa o pedido do usu√°rio em busca de potenciais riscos
        
        Args:
            user_input: Texto do usu√°rio
            context: Contexto adicional da conversa
        
        Returns:
            RiskAssessment se risco identificado, None caso contr√°rio
        """
        user_input_lower = user_input.lower()
        
        # Verificar cada categoria de risco
        for category, patterns_list in self.risk_patterns.items():
            for pattern_group in patterns_list:
                for pattern in pattern_group["patterns"]:
                    if re.search(pattern, user_input_lower):
                        logger.info(f"Risco identificado: {category.value} - {pattern}")
                        
                        return RiskAssessment(
                            level=pattern_group["level"],
                            category=category,
                            concern=pattern_group["concern"],
                            alternative_suggestion=pattern_group.get("suggestion"),
                            reasoning=f"Detectei um padr√£o de risco relacionado a {category.value}"
                        )
        
        return None
    
    def generate_concern_response(self, assessment: RiskAssessment, personality: str = "neutra") -> str:
        """
        Gera uma resposta de preocupa√ß√£o baseada na avalia√ß√£o de risco
        
        Args:
            assessment: Avalia√ß√£o do risco
            personality: Personalidade do ASTRA
        
        Returns:
            Resposta formatada com preocupa√ß√£o e sugest√£o
        """
        # Escolher tom baseado na personalidade
        if personality == "amig√°vel":
            concern_prefix = "Olha, como seu amigo, "
            suggestion_prefix = "Que tal "
        elif personality == "formal":
            concern_prefix = "Devo expressar que "
            suggestion_prefix = "Recomendo que "
        elif personality == "casual":
            concern_prefix = "Cara, "
            suggestion_prefix = "E se voc√™ "
        else:  # neutra
            concern_prefix = ""
            suggestion_prefix = ""
        
        # Construir resposta
        response_parts = []
        
        # Expressar preocupa√ß√£o
        if assessment.level == RiskLevel.CRITICAL:
            response_parts.append(f"üö® {concern_prefix}{assessment.concern}.")
        elif assessment.level == RiskLevel.HIGH:
            response_parts.append(f"‚ö†Ô∏è {concern_prefix}{assessment.concern}.")
        elif assessment.level == RiskLevel.MEDIUM:
            response_parts.append(f"üòü {concern_prefix}{assessment.concern}.")
        else:
            response_parts.append(f"ü§î {concern_prefix}{assessment.concern}.")
        
        # Adicionar racioc√≠nio se dispon√≠vel
        if assessment.reasoning:
            response_parts.append(f"\n{assessment.reasoning}.")
        
        # Adicionar sugest√£o alternativa
        if assessment.alternative_suggestion:
            response_parts.append(f"\n\nüí° {suggestion_prefix}{assessment.alternative_suggestion}")
        
        # Adicionar oferecimento de ajuda
        if assessment.level.value >= RiskLevel.MEDIUM.value:
            response_parts.append("\n\nPosso ajudar voc√™ a encontrar uma solu√ß√£o melhor?")
        
        return "".join(response_parts)
    
    def should_decline_request(self, assessment: RiskAssessment) -> bool:
        """
        Determina se o ASTRA deve se recusar a ajudar com o pedido
        
        Args:
            assessment: Avalia√ß√£o do risco
        
        Returns:
            True se deve recusar, False caso contr√°rio
        """
        return assessment.level.value >= RiskLevel.HIGH.value
    
    def get_alternative_help(self, category: RiskCategory) -> str:
        """
        Oferece tipos alternativos de ajuda baseados na categoria
        
        Args:
            category: Categoria do risco identificado
        
        Returns:
            Sugest√£o de ajuda alternativa
        """
        alternatives = {
            RiskCategory.HEALTH: "Posso ajudar voc√™ a encontrar informa√ß√µes sobre h√°bitos saud√°veis ou localizar profissionais de sa√∫de na sua regi√£o.",
            RiskCategory.SAFETY: "Posso sugerir alternativas mais seguras ou ajudar voc√™ a planejar uma abordagem mais cautelosa.",
            RiskCategory.LEGAL: "Posso ajudar voc√™ a pesquisar alternativas legais ou encontrar orienta√ß√£o jur√≠dica.",
            RiskCategory.FINANCIAL: "Posso ajudar com dicas de educa√ß√£o financeira ou encontrar recursos para planejamento financeiro.",
            RiskCategory.RELATIONSHIP: "Posso sugerir formas construtivas de comunica√ß√£o ou recursos para relacionamentos saud√°veis.",
            RiskCategory.ADDICTION: "Posso ajudar voc√™ a encontrar recursos de apoio ou atividades alternativas saud√°veis.",
            RiskCategory.PRIVACY: "Posso ensinar sobre pr√°ticas de seguran√ßa digital e prote√ß√£o da privacidade."
        }
        
        return alternatives.get(category, "Posso ajudar voc√™ a encontrar uma abordagem mais segura e construtiva.")

# Inst√¢ncia global para uso em outros m√≥dulos
ethical_analyzer = EthicalAnalyzer()
